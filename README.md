# Contriever
This codebase is an implementation of the paper: **Unsupervised Dense Retrieval with Contrastive Learning**  
https://arxiv.org/pdf/2112.09118

## Requirements
- BEIR library

## Datasets  
- The model is trained on preprocessed wikipedia dataset
- For evaluation, we use BEIR benchmark datasets like Natural Questions and TriviaQA
- All the raw and processed dataset files are available in the drive attached in datasets folder
- The code to download and preprocess the datasets are also present in the main script train_wikipedia.py

## Training
- train_wikipedia.py is the main train script
- The folder src contains all the required helper modules
- It also contains the models for 2 contrastive learning strategies- MoCo and InBatch
- The learning strategy implemented here is MoCo

## Document similarity
- The cosine similarity scores between different documents can be calculated using the document embeddings generated by the encoder of the trained model

## Evaluation comparison
- We compare the evaluation of the model using 2 pipelines
  - Bi-encoder of the trained model
  - Bi-encoder of the trained model + Cross Encoder Reranking
